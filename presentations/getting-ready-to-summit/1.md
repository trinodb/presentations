<!--
https://trino.io/blog/2024/10/07/sql-basecamps

The second episode SQL Basecamp 2 â€“ Getting ready to summit builds on the
foundation established in episode 1. Data has moved into the lakehouse, powered
by Trino, and more data is added and changed as part of normal operation. In
this episode Martin and myself look at maintaining the data in a healthy state
and explore some tips and tricks for querying data. For example, we look at data
management with procedures, analyzing data with window functions, and examine
more complex structural data.
-->

# Overview

A journey through different topics

* Dealing with change
* A few more tips for data maintenance
* Complex types like JSON, map, array, and row 
* Window functions and `match_recognize`
* A glimpse at views

-vertical
## Dealing with change

* One of the core issues on data lakes
* Time and cost are core issues
* Limited with Hive, better support on lakehouses
* Careful about syntax, compare `CALL` and `ALTER TABLE EXECUTE`

-vertical
## Hive schema evolution

* Helpful for legacy usage
* [Rather limited](https://trino.io/docs/current/connector/hive.html#schema-evolution)
* Column order is significant

-vertical
## Data maintenance tips

Maintenance is critical for performance

* Compaction
* Snapshot removal
* File format change
* Data removal
* Migration to other storage

-vertical
## MERGE statement

* Only available for object storage connectors
* Powerful tool to insert, update, and delete
* Especially for regular changes
* Can be emulated with multiple other statements
* Details in other training ([1](../sql-data-mgt/index.html#/2/12), [2](../sql-advanced-overview/index.html#/1/8))

-notes 
* Coming to other connectors

-vertical
## Structural data types

* Content of individual columns
* JSON
* Map, Array, Row

-vertical
## JSON overview

* JSON is a very common format
* REST API, logging, and much more
* Often in context of "NoSQL" databases
* Trino has very rich support 
* Discussed before [in detail](../sql-adv-analytics/index.html#/2)

-vertical
## New `json_table` support

* Function that returns a table structure
* From input of a JSON value
* Part of SQL standard

-vertical
## `json_table` example

```sql
SELECT
  *
FROM
  json_table(
    '[
        {"id":1,"name":"Africa","wikiDataId":"Q15"},
        {"id":2,"name":"Americas","wikiDataId":"Q828"},
        {"id":3,"name":"Asia","wikiDataId":"Q48"},
        {"id":4,"name":"Europe","wikiDataId":"Q51"}
    ]',
    'strict $' COLUMNS (
        NESTED PATH 'strict $[*]' COLUMNS (
        id integer path 'strict $.id',
        name varchar path 'strict $.name',
        wiki_data_id varchar path 'strict $."wikiDataId"'
        )
    )
  );
```

-vertical
## Structural types

Trino data types

* `ARRAY` - list of values, same type
* `MAP` - list of key-value pairs
* `ROW` - list of values, different types

Often map to data source types.

-vertical
## ARRAY

```sql
SELECT ARRAY[1, 1.2, 4];
-- [1.0, 1.2, 4.0]
```

* Construct from values
* Subscript operator access `[]` to access elements
* Concatenation with `||`
* [Lots of functions](https://trino.io/docs/current/functions/array.html)

-notes
Let's talk about some practical usage 

-vertical
## MAP

```sql
SELECT MAP(ARRAY['key1', 'key2', 'key3'], 
           ARRAY['v1',   'v2',   'v3']);
-- {key1=v1, key2=v2, key3=v3}
```

* Construct from values 
* Subscript operator `[]` to access elements
* [Lots of functions](https://trino.io/docs/current/functions/map.html) 

-vertical
## ROW

```sql
SELECT CAST(ROW(1, 2e0, 'foo') AS 
    ROW(x BIGINT, y DOUBLE, info VARCHAR));
```

* Construct from value with optional field names
* Like a row of a table
* Subscript operator `[]` to access elements

-notes
Can you combine rows to a table?

-vertical
## Views

* Very powerful concept
* Allows hiding of complexity
* More details in [other material](../sql-data-mgt/index.html#/3)

-vertical
## Materialized views

* An extension of views
* Includes actual data storage
* Users need to deal with staleness and refresh
* Complex and limited to Iceberg
