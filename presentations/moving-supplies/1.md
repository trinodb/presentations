<!--

https://trino.io/blog/2024/10/07/sql-basecamps

In the first episode SQL basecamp 1 â€“ Moving supplies, David and Dain will help
me provide an overview of the wide range of possibilities when it comes to
moving data to Trino and moving data with Trino.

We specifically look at the strengths of Trino for running your data lakehouse
and migrating to it from legacy data lakes or other systems. SQL skills
discussed include tips for creating schemas and tables, adding and updating
data, and inspecting metadata. We talk about table procedures for data
management and also cover some operational aspects. For example, we talk about
the right configuration in your catalogs for your object storage, specifically
the new file system support in Trino.
-->

# Overview

* What is a lakehouse
* Components of a lakehouse
* Trino as lakehouse query engine
* Moving data in and around
* Management and maintenance

We create a foundation for your own experiments and learning.

-vertical
## What is a lakehouse

Data warehouse

* RDBMS for lots of data
* Structured data
* Typically strict management

Data lake

* Object storage for lots of data
* Often unstructured, or less structured
* Looser management

A data lakehouse combines the benefits of both.

-vertical
## Components of a lakehouse

* Files for data
* Object storage for hosting files
* Metastore for metadata
* Modern storage format
* Query engine for analytics and management

-vertical
## Files

* Originally CSV, JSON, and others
* Now columnar formats like Apache Parquet and Apache ORC
* Others, including Apache Avro

-vertical
## Object storage

* HDFS from Apache Hadoop originally
* Amazon S3
* S3-compatible
* Azure Storage
* Google Cloud Storage

-vertical
## Object storage and files in Trino

* Modern [file system support](https://trino.io/docs/current/object-storage.html) in Trino
* High-performance reader and writer libraries for Parquet and ORC
* Efficient compression libraries

-notes
* Migrate off the legacy system from Hive/HDFS now!

-vertical
## Metastore

Manage metadata about the object storage, files, and data in them:

* Data about schemas, tables, and other objects
* Columns and data types
* Pointers to object storage locations
* Various other features depending on metastore

-vertical
## Available metastores

* Hive Metastore Service (HMS)
    * Service and database
    * Thrift protocol
* Amazon Glue
* Databricks Unity
* Apache Polaris
* Nessie

-vertical
## Catalogs? 

The usage of the terms *catalog* is confusing:

Trino

* Catalog is configuration to access a data source

Iceberg

* Catalog is the metastore
* REST, JDBC and other catalogs supported
* Iceberg stores most metadata in file system

-vertical
## Data catalogs

* Often used term for related systems
* Larger scope 
* Authentication, access control, and governance
* Custom metadata, search, and data discoverability

Differences are getting smaller.

-vertical
## Table format

The original and the contenders

* Apache Hive
* Apache Iceberg 
* Delta Lake
* Apache Hudi

-vertical
## Table format

* Defines how data and metadata are organized and managed.
* Uses files in object storage and metastores.

-vertical
## Metastores and table formats

* Metadata in the metastore
* Metadata also in object storage
* For example AVRO files in Iceberg
* Hive - lots of metadata in metastore
* Iceberg - nearly all metadata in files

-vertical
## Side note about immutability

* Important concept to keep in mind for table formats
* Data files are typically immutable
* Changes are written to new files
* And metadata is updated
* Enables snapshots and time travel

-vertical
## Downsides

* Over time that leads to bloat
* Need for clean up
* Compaction
* Snapshot removal

-vertical
## Reality about table formats

* What is the best table format? It depends.
* Legacy deployments with Hive continue for a long time.
* Get used to dealing with interoperability.
* Migrations and co-existence are common.

It's is not necessarily getting easier.

-vertical
## Table formats and metastores in Trino
 
 * Hive, Iceberg, Delta Lake, and Hudi connectors
 * HMS and Glue support in all connectors
 * Nessie, Iceberg JDBC and Iceberg REST in Iceberg
 * Unity in Delta Lake
 * Unity and Polaris in Iceberg

-vertical
## A world full of choices

* Multiple choices for everything.
* Common to deal with multiple impacting factors.
* There is no single, perfect choice or solution.

-notes 
But Starburst Galaxy and other products are easier than 
doing it all yourself with Trino, if less flexible.

-vertical
## Setting up a Trino lakehouse

* Object storage location and provider
* Impact on deployment location
* Relevant for file system support
* Metastore choices vary
* Catalogs with object storage connectors
* And potentially catalogs for other data sources

-horizontal
## Trino lakehouse

Practical tips and tricks

-vertical
## Examples for moving data in

* File creation on object storage in CSV, JSON or even log files
* Legacy data lake on Hive with migration to lakehouse
* Combination of lakehouse with warehouse and other RDBMS
* Data streaming into object storage

-vertical
## New schema and table

* `CREATE SCHEMA ...` with schema properties
* `CREATE TABLE ...` with table properties
* Include partitioning info
* Object storage location derived from catalog

-vertical
## Partitioning and bucketing

* Splits up data in separate locations
* Improves query performance

-vertical
## Common scenario - existing table

* Details are available in object storage and metastore
* Typically from other query engine
* `CALL example.system.register_table ...` in Iceberg and Delta Lake
* `CREATE TABLE ... WITH ( external_location = '...` in Hive
* Time to discuss managed and external tables.

-notes
* Made easy with schema discovery in Starburst Galaxy.

-vertical
## Existing Hive table

Migrate to Hive table to Iceberg:

```sql
CALL example.system.migrate(
    schema_name => 'testdb',
    table_name => 'customer_orders',
    recursive_directory => 'true');
```

-vertical
## Existing Hive table

Add more files from Hive table

```sql
ALTER TABLE testdb.iceberg_customer_orders 
EXECUTE example.system.add_files_from_table(
    schema_name => 'testdb',
    table_name => 'hive_customer_orders');
```

-vertical
## Existing files

Add files to Iceberg

```sql
ALTER TABLE testdb.iceberg_customer_orders 
EXECUTE example.system.add_files(
    location => 's3://my-bucket/a/path',
    format => 'ORC');
```

-vertical
## Migrating data into catalog

* More generic approaches
* Initially `CREATE TABLE AS SELECT ...`
* From RDBMS, Hive, or others to lakehouse
* `INSERT`, `UPDATE`, `DELETE`, not yet `MERGE` depending on connector
* Use table properties to change file format, partitioning, ...
* Potentially changes storage completely, including table format and metastore

-vertical
## Constant flow of new data

* Streaming systems like Kafka 
* Many other applications
* Data dumps into object storage
* Common scenario
* Requires constant metadata updates
* External updates necessitate maintenance in Trino

-notes
* Starburst Galaxy streaming ingest makes that easy

-vertical
## Once the data is there

There are a lot of tasks besides querying the data.

-vertical
## Inspection time

* Hidden tables
* Access metadata from table format
* Varies per connector
* Use `$partitions`, `$manifests`, `$snapshots`,...

```sql
SELECT * FROM "test_table$files";
```

-vertical
## Metadata fixes in Hive

```sql
CALL example.system.sync_partition_metadata(...);
CALL example.system.register_partition(...);
```

-vertical
## Table maintenance

* `ANALYZE` updates statistics
* Important for cost-based optimizer

```sql
ANALYZE example.default.customers 
WITH (partitions=ARRAY[ARRAY['CA', 'San Francisco']]);
```

-vertical
## Table procedures in Iceberg

```sql
ALTER TABLE test EXECUTE optimize(file_size_threshold => '128MB'); -- compaction
ALTER TABLE test EXECUTE expire_snapshots(retention_threshold => '7d');
ALTER TABLE test EXECUTE remove_orphan_files(retention_threshold => '7d');
```

-vertical
## Table properties

```sql
ALTER TABLE test 
SET PROPERTIES partitioning=ARRAY[<existing partition columns>, 'my_new_partition_column'];
```

-horizontal
## Final thoughts

-vertical
## Interoperability is hard 

* Lakehouse uses many technologies
* And combines usage
* Including multiple query engines
* What works will vary
* And evolve over time

-vertical
## Multiple query engines

* Common scenario
* Trino for analytics
* Other engine for OLTP and real-time workloads
* Other engine for Python, batch, ML, ...
* Advantages come with lots of added complexity

> Often Trino can do more or do it all.

-vertical
## Summary

Trino is a great query engine to build a lakehouse.
