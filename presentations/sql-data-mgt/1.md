# On the other side of analytics

... you find the need to manipulate data.

-vertical
## Quick overview

* Data source
* Catalog
* Schema
* Table
* Columns and rows

> Data access uses `catalog.schema.object`

-vertical
## Access and capabilities

So far we learned about analytics:

* [Globally available statements](https://trino.io/docs/current/language/sql-support.html#globally-available-statements)
* [Read operations](https://trino.io/docs/current/language/sql-support.html#read-operations)

Supported by all data sources and Trino connectors.

-vertical
## Beyond analytics

[Write operations](https://trino.io/docs/current/language/sql-support.html#read-operations) include:

* Catalog management
* Schema and table management
* **Data management**
* View management
* Materialized view management
* Routine management

-vertical
## More access and capabilities

For successful write, the following must be true:

* Data source supports write operations.
* Trino connector supports write operations.
* Desired data types are mapped in connector.
* Credentials in catalog provide sufficient access.
* Trino security configuration allows writes.

-vertical
## How to check

* SQL support section in connector documentation
* Security configuration of the data source
* Trino access control disabled or configured for writes
    * Not `read-only` [system access
      control](https://trino.io/docs/current/security/built-in-system-access-control.html)
    * Not restricted system or catalog access in [file-based access
      control](https://trino.io/docs/current/security/file-system-access-control.html)

-vertical
## Catalogs

* Configuration to access a data source
* Typically configured by catalog properties files or user interface
* New dynamic catalogs as **experimental** feature

-vertical
## Create catalog with SQL

In `config.properties`:

```
catalog.management=dynamic
```

In your client:

```sql
CREATE CATALOG brain USING memory;
SHOW CATALOGS;
```

Note the catalog name and connector name.

-vertical
## More complex catalog

```sql
CREATE CATALOG example USING postgresql
WITH (
  "connection-url" = 'jdbc:pg:localhost:5432',
  "connection-user" = '${ENV:POSTGRES_USER}',
  "connection-password" = '${ENV:POSTGRES_PASSWORD}',
  "case-insensitive-name-matching" = 'true'
);
```

Note the usage of quotes and secrets.

-vertical
## Intermission

* Catalog creation really is just preparation.
* Important initial  task, but later not that common.

> Before we add data, we need to discuss data sources.

-vertical
## Object storage data sources are very different

* Hive, Iceberg, Delta Lake, and Hudi
* Files in a folder structure
* Ideally columnar files, like ORC or Parquet
* Use different table formats
* Require separate metadata storage
* Storage of data and metadata only, often on cloud storage

Object storage requires a query engine like Trino.

-vertical
## Object storage metadata

* In separate metadata files in object storage
* And in metastore
  * Hive Metastore Service (HMS) via Thrift protocol
  * Glue catalog
  * REST, JDBC, and Nessie catalogs for Iceberg

-vertical
## Traditional RDBMS and other data sources

* Typically manage query and storage
* Include a query engine
* Often support some SQL natively
* JDBC driver or API access typically

Trino acts as additional query engine and user interface for multiple data
sources.

-vertical
## Data source differences

* Ideally data source is irrelevant
* Practically some differences show in Trino
* Connectors with different SQL support
* Type availability and mapping
* Different convenience and advanced features

-vertical
## Schema

* Equivalent to a database, schema, folder in object storage
* Depending on data source

```text
CREATE SCHEMA [ IF NOT EXISTS ] schema_name
[ AUTHORIZATION ( user | USER user | ROLE role ) ]
[ WITH ( property_name = expression [, ...] ) ]
```

Note authorization and schema properties.

-vertical
## Create schema examples

```sql
CREATE SCHEMA brain.demo;

CREATE SCHEMA lake.marketing
  WITH (
    location = 's3://...storage/lake/marketing'
  );
```

Location is an optional schema property.

-vertical
## Tables

* Columns with data types
* Rows of values for each column
* Located in a schema in a catalog
* Consider type mapping of connector

-vertical
## Create table

Simplest example:

```sql
CREATE TABLE person (
  firstname varchar(50),
  lastname varchar(50),
  dob date
);
```

-vertical
## Full create table syntax

```text
CREATE [ OR REPLACE ] TABLE [ IF NOT EXISTS ]
table_name (
  { column_name data_type [ NOT NULL ]
      [ COMMENT comment ]
      [ WITH ( property_name = expression [, ...] ) ]
  | LIKE existing_table_name
      [ { INCLUDING | EXCLUDING } PROPERTIES ]
  }
  [, ...]
)
[ COMMENT table_comment ]
[ WITH ( property_name = expression [, ...] ) ]
```

Note `NOT NULL`, comments, table properties, and existing table usage.

-vertical
## Inspection time

```text
SHOW CATALOGS;
SHOW SCHEMAS FROM brain;
SHOW TABLES FROM brain.demo;
SHOW CREATE TABLE brain.demo.person;
DESCRIBE brain.demo.person;
```

-vertical
## Inspection time again

Directly looking at the metadata in system catalog and catalog
`information_schema`:

```sql
SELECT * FROM system.metadata.catalogs;

SELECT * FROM brain.information_schema.columns
  WHERE table_schema = 'demo'
  AND table_name = 'person';
```

`DESCRIBE` and `SHOW CREATE` statements for convenience.

-vertical
## Metadata

Schema, table, and column information:

* Varies a lot per connector and data source
* Schema, table, and column properties and comments
* Available in `system` catalog, for example `system.metadata.table_properties`
* Typically more important for object storage
* Properties set in `WITH` clause of `CREATE SCHEMA|TABLE`
* Often optional, allow fine-grained control

-vertical
## Hive metadata

Available in hidden metadata table and hidden fields:

```sql
SELECT * FROM catalog.schema."tablename$properties";
SELECT * FROM catalog.schema."tablename$partitions";
SELECT *, "$path", "$file_size", "$partition" FROM catalog.schema.tablename;
```

-vertical
## Hive metadata

Important for data organization:

```sql
CREATE TABLE
...
  ds DATE,
  country VARCHAR
WITH (
    location = 's3://.../bucket/'
    format='ORC',
    partitioned_by=  ARRAY['ds', 'country']
);
```

Lots more information is [available in the
documentation](https://trino.io/docs/current/connector/hive.html#schema-and-table-management).

-vertical
## Iceberg table properties

Similar to Hive, but [even more feature rich](https://trino.io/docs/current/connector/iceberg.html#schema-and-table-management).

